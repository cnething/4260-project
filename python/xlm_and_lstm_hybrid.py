# -*- coding: utf-8 -*-
"""XLM and LSTM hybrid

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K9_L8RckKipDyw4dc1xzgxewds8V4Wmg
"""

!pip install transformers datasets tqdm scikit-learn pandas numpy torch

import pandas as pd
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import XLMRobertaModel, XLMRobertaTokenizer, get_linear_schedule_with_warmup
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm.notebook import tqdm  # Using notebook tqdm for Colab
import warnings
import gc
import os
from datetime import datetime

"""# Check if GPU is available

"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using {device} device")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
    print(f"Memory Reserved: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB")

# For reproducibility
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    # T4 GPU optimizations
    torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes
    torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere
    torch.backends.cudnn.allow_tf32 = True

# Create directory for saving models and results
os.makedirs('model_outputs', exist_ok=True)
MODEL_PATH = 'model_outputs/best_xlm_roberta_lstm_model.pt'

# Mixed precision training for faster computation
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

# Function to clear GPU cache
def clear_gpu_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

"""Load and prepare data"""

def load_data(train_path, test_path=None, validation_split=0.1):
    """Load and prepare the data for the model"""
    print(f"Loading data from {train_path}...")
    train_df = pd.read_csv(train_path)
    print(f"Train data shape: {train_df.shape}")

    # Map label strings to integers if needed
    if 'label' in train_df.columns and train_df['label'].dtype == 'object':
        label_map = {'entailment': 0, 'neutral': 1, 'contradiction': 2}
        train_df['label'] = train_df['label'].map(label_map)

    # Split data if no test set provided
    if test_path:
        print(f"Loading test data from {test_path}...")
        test_df = pd.read_csv(test_path)
        print(f"Test data shape: {test_df.shape}")

        # If validation split is needed with separate test set
        if validation_split > 0:
            train_df, val_df = train_test_split(
                train_df,
                test_size=validation_split,
                random_state=SEED,
                stratify=train_df['label'] if 'label' in train_df.columns else None
            )
            print(f"Split train data: {train_df.shape}, validation: {val_df.shape}")
            return train_df, val_df, test_df
        return train_df, test_df
    else:
        # Split training data into train and validation
        train_df, val_df = train_test_split(
            train_df,
            test_size=validation_split,
            random_state=SEED,
            stratify=train_df['label'] if 'label' in train_df.columns else None
        )
        print(f"Split train data: {train_df.shape}, validation: {val_df.shape}")
        return train_df, val_df

"""Optimized dataset class for more efficient memory usage"""

class NLIDataset(Dataset):
    def __init__(self, texts, labels=None, tokenizer=None, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        premise = str(self.texts[idx]['premise'])
        hypothesis = str(self.texts[idx]['hypothesis'])

        encoding = self.tokenizer.encode_plus(
            premise,
            hypothesis,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        item = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }

        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)

        return item

"""XLM-RoBERTa + LSTM Hybrid Model optimized for T4 GPU"""

class XLMRoBERTaLSTM(nn.Module):
    def __init__(self, num_classes=3, dropout_rate=0.3, lstm_hidden_size=256, freeze_bert_layers=None):
        super(XLMRoBERTaLSTM, self).__init__()

        # Load pre-trained XLM-RoBERTa model
        self.xlm_roberta = XLMRobertaModel.from_pretrained('xlm-roberta-base')
        hidden_size = self.xlm_roberta.config.hidden_size

        # Freeze specific layers of XLM-RoBERTa to reduce memory usage and speed up training
        if freeze_bert_layers is not None:
            self._freeze_bert_layers(freeze_bert_layers)

        # LSTM layer - optimized for T4 GPU
        self.lstm = nn.LSTM(
            input_size=hidden_size,
            hidden_size=lstm_hidden_size,
            num_layers=2,
            batch_first=True,
            bidirectional=True,
            dropout=0.2 if dropout_rate > 0 else 0
        )

        # Dropout to prevent overfitting
        self.dropout = nn.Dropout(dropout_rate)

        # Final classification layer
        self.classifier = nn.Linear(lstm_hidden_size * 2, num_classes)  # *2 for bidirectional

    def _freeze_bert_layers(self, num_layers):
        """Freeze the first n layers of BERT to save memory and computation"""
        if num_layers == 0:
            return

        modules = [self.xlm_roberta.embeddings]
        if num_layers > 0:
            modules.extend(self.xlm_roberta.encoder.layer[:num_layers])

        for module in modules:
            for param in module.parameters():
                param.requires_grad = False

        print(f"Froze {num_layers} layers of XLM-RoBERTa")

    def forward(self, input_ids, attention_mask):
        # Pass input through XLM-RoBERTa with attention mask
        outputs = self.xlm_roberta(input_ids=input_ids, attention_mask=attention_mask)

        # Get sequence output from XLM-RoBERTa
        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]

        # Apply LSTM over the sequence
        lstm_output, (hidden, _) = self.lstm(sequence_output)

        # Concatenate the final forward and backward hidden states
        hidden_forward = hidden[-2, :, :]
        hidden_backward = hidden[-1, :, :]
        hidden_concat = torch.cat((hidden_forward, hidden_backward), dim=1)

        # Apply dropout
        hidden_concat = self.dropout(hidden_concat)

        # Final classification
        logits = self.classifier(hidden_concat)

        return logits

"""Enhanced training function with mixed precision for T4 GPU"""

# Enhanced training function with mixed precision for T4 GPU
def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs, device, patience=2):
    # Loss function
    criterion = nn.CrossEntropyLoss()

    # For mixed precision training
    scaler = GradScaler()

    # Track best validation metrics
    best_val_accuracy = 0
    best_val_f1 = 0
    best_epoch = 0
    no_improvement = 0

    # Create log file
    log_file = open('./model_outputs/training_log.txt', 'w')
    log_file.write(f"Training started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    log_file.write(f"Model: XLM-RoBERTa + LSTM Hybrid\n")
    log_file.write(f"Device: {device}\n\n")

    # Training loop
    for epoch in range(num_epochs):
        epoch_start_time = datetime.now()
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        log_file.write(f"\nEpoch {epoch+1}/{num_epochs}\n")

        # Training phase
        model.train()
        train_loss = 0
        progress_bar = tqdm(train_loader, desc="Training")

        for batch_idx, batch in enumerate(progress_bar):
            # Move batch to device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass with mixed precision
            optimizer.zero_grad()

            with autocast():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                loss = criterion(outputs, labels)

            # Backward pass with scaled gradients
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            # Update weights with scaled gradients
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

            train_loss += loss.item()

            # Update progress bar
            progress_bar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'lr': f"{scheduler.get_last_lr()[0]:.2e}"
            })

            # Free up memory
            if batch_idx % 10 == 0 and batch_idx > 0:
                clear_gpu_memory()

        avg_train_loss = train_loss / len(train_loader)
        print(f"Average training loss: {avg_train_loss:.4f}")
        log_file.write(f"Average training loss: {avg_train_loss:.4f}\n")

        # Validation phase
        model.eval()
        val_loss = 0
        val_preds = []
        val_true = []

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                # Use mixed precision for inference as well
                with autocast():
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                    loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, preds = torch.max(outputs, dim=1)

                val_preds.extend(preds.cpu().tolist())
                val_true.extend(labels.cpu().tolist())

        avg_val_loss = val_loss / len(val_loader)

        # Calculate validation metrics
        val_accuracy = accuracy_score(val_true, val_preds)
        val_f1 = f1_score(val_true, val_preds, average='macro')

        print(f"Validation Loss: {avg_val_loss:.4f}")
        print(f"Validation Accuracy: {val_accuracy:.4f}")
        print(f"Validation F1 Score: {val_f1:.4f}")

        log_file.write(f"Validation Loss: {avg_val_loss:.4f}\n")
        log_file.write(f"Validation Accuracy: {val_accuracy:.4f}\n")
        log_file.write(f"Validation F1 Score: {val_f1:.4f}\n")

        # Print detailed classification report
        report = classification_report(val_true, val_preds, target_names=['entailment', 'neutral', 'contradiction'])
        print("\nClassification Report:")
        print(report)
        log_file.write("\nClassification Report:\n")
        log_file.write(report + "\n")

        # Save best model based on F1 score and accuracy
        if val_f1 > best_val_f1 or (val_f1 == best_val_f1 and val_accuracy > best_val_accuracy):
            best_val_accuracy = val_accuracy
            best_val_f1 = val_f1
            best_epoch = epoch + 1

            # Save model
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_accuracy': val_accuracy,
                'val_f1': val_f1,
            }, MODEL_PATH)

            print(f"Saved best model at epoch {epoch+1} with F1: {val_f1:.4f}, Accuracy: {val_accuracy:.4f}")
            log_file.write(f"Saved best model at epoch {epoch+1} with F1: {val_f1:.4f}, Accuracy: {val_accuracy:.4f}\n")
            no_improvement = 0
        else:
            no_improvement += 1
            print(f"No improvement for {no_improvement} epochs")
            log_file.write(f"No improvement for {no_improvement} epochs\n")

        # Early stopping
        if no_improvement >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            log_file.write(f"Early stopping triggered after {epoch+1} epochs\n")
            break

        # Calculate epoch time
        epoch_time = datetime.now() - epoch_start_time
        print(f"Epoch completed in: {epoch_time}")
        log_file.write(f"Epoch completed in: {epoch_time}\n")

        # Clear GPU memory after each epoch
        clear_gpu_memory()

    # These lines should be at the same indentation level as the for loop
    log_file.write(f"\nBest model was from epoch {best_epoch} with F1: {best_val_f1:.4f}, Accuracy: {best_val_accuracy:.4f}\n")
    log_file.write(f"Training completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    log_file.close()

    return best_epoch, best_val_accuracy, best_val_f1

"""Calculate validation metrics

Optimized inference function for test data
"""

def predict(model, test_loader, device, output_path='submission.csv', label_map=None):
    model.eval()
    predictions = []
    softmax = nn.Softmax(dim=1)
    confidence_scores = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Predicting"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            # Use mixed precision for inference
            with autocast():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)

            probs = softmax(outputs)
            confidence, preds = torch.max(probs, dim=1)

            predictions.extend(preds.cpu().tolist())
            confidence_scores.extend(confidence.cpu().tolist())

            # Clear GPU memory periodically
            if len(predictions) % 1000 == 0:
                clear_gpu_memory()

    # Map predictions to labels if provided
    if label_map:
        predicted_labels = [label_map[pred] for pred in predictions]
        return predicted_labels, confidence_scores

    return predictions, confidence_scores

# Function to create submission file with confidence scores
def create_submission(test_df, predictions, confidence_scores, output_path='submission.csv', include_confidence=False):
    if include_confidence:
        submission_df = pd.DataFrame({
            'id': test_df['id'],
            'prediction': predictions,
            'confidence': confidence_scores
        })
    else:
        submission_df = pd.DataFrame({
            'id': test_df['id'],
            'prediction': predictions
        })

    submission_df.to_csv(output_path, index=False)
    print(f"Submission file created at {output_path}!")
    return submission_df

"""T4 GPU optimized main function"""

def main():
    # Set batch sizes based on available GPU memory
    TRAIN_BATCH_SIZE = 16
    EVAL_BATCH_SIZE = 32
    MAX_SEQ_LENGTH = 128
    EPOCHS = 5
    LEARNING_RATE = 2e-5
    FREEZE_LAYERS = 6  # Freeze first 6 layers to save memory


    # Set file paths - adjust these to your Google Drive paths if needed
    TRAIN_PATH = 'train.csv'
    TEST_PATH = 'test.csv'

    # Load data
    train_df, val_df = load_data(TRAIN_PATH)
    test_df = pd.read_csv(TEST_PATH)

    # For memory optimization in Colab
    clear_gpu_memory()

    # Initialize tokenizer
    print("Loading XLM-RoBERTa tokenizer...")
    tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')

    print("Creating datasets...")
    # Create datasets
    train_dataset = NLIDataset(
        texts=train_df[['premise', 'hypothesis']].to_dict('records'),
        labels=train_df['label'].tolist() if 'label' in train_df.columns else None,
        tokenizer=tokenizer,
        max_length=MAX_SEQ_LENGTH
    )

    val_dataset = NLIDataset(
        texts=val_df[['premise', 'hypothesis']].to_dict('records'),
        labels=val_df['label'].tolist() if 'label' in val_df.columns else None,
        tokenizer=tokenizer,
        max_length=MAX_SEQ_LENGTH
    )

    test_dataset = NLIDataset(
        texts=test_df[['premise', 'hypothesis']].to_dict('records'),
        tokenizer=tokenizer,
        max_length=MAX_SEQ_LENGTH
    )

    print("Creating data loaders...")
    # Create optimized data loaders with num_workers for parallel loading
    train_loader = DataLoader(
        train_dataset,
        batch_size=TRAIN_BATCH_SIZE,
        shuffle=True,
        num_workers=2,
        pin_memory=True  # Speeds up host to GPU transfers
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=EVAL_BATCH_SIZE,
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=EVAL_BATCH_SIZE,
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )

    # Clear memory before initializing model
    clear_gpu_memory()
    print(f"Memory before model init: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")

    print("Initializing model...")
    # Initialize model
    model = XLMRoBERTaLSTM(
        num_classes=3,
        dropout_rate=0.3,
        lstm_hidden_size=256,
        freeze_bert_layers=FREEZE_LAYERS
    )
    model = model.to(device)

    print(f"Memory after model init: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")

# Initialize optimizer and scheduler with weight decay differentiation
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {
            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],
            'weight_decay': 0.01
        },
        {
            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],
            'weight_decay': 0.0
        }
    ]

    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)

    # Calculate total training steps
    total_steps = len(train_loader) * EPOCHS

    # Create scheduler with warmup
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_steps * 0.1),
        num_training_steps=total_steps
    )

    # Train model
    print("\nStarting training...")
    best_epoch, best_acc, best_f1 = train_model(
        model,
        train_loader,
        val_loader,
        optimizer,
        scheduler,
        num_epochs=EPOCHS,
        device=device,
        patience=2  # Early stopping after 2 epochs without improvement
    )

    print(f"\nTraining completed! Best epoch: {best_epoch} with accuracy: {best_acc:.4f} and F1: {best_f1:.4f}")

    # Clear memory before inference
    clear_gpu_memory()

    # Load best model for prediction
    print("Loading best model for inference...")
    checkpoint = torch.load(MODEL_PATH)
    model.load_state_dict(checkpoint['model_state_dict'])

    # Make predictions on test set
    print("\nGenerating predictions on test set...")
    label_map_reverse = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}
    predicted_labels, confidence_scores = predict(
        model,
        test_loader,
        device,
        label_map=label_map_reverse
    )

    # Create submission files
    print("Creating submission files...")
    # Regular submission
    submission_df = create_submission(
        test_df,
        predicted_labels,
        confidence_scores,
        output_path='model_outputs/submission.csv'
    )

    # Submission with confidence scores for analysis
    confidence_df = create_submission(
        test_df,
        predicted_labels,
        confidence_scores,
        output_path='model_outputs/submission_with_confidence.csv',
        include_confidence=True
    )

    print("\nAll done! Check the 'model_outputs' directory for results and the best model.")

# Run the code with Google Colab compatibility
if __name__ == "__main__":
    # Print available GPU info
    print("\n===== SYSTEM INFO =====")
    # Check CUDA version
    if torch.cuda.is_available():
        print(f"CUDA Version: {torch.version.cuda}")
        print(f"PyTorch CUDA: {torch.cuda.is_available()}")
        print(f"Number of GPUs: {torch.cuda.device_count()}")

        # Check GPU memory
        print(f"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        print(f"Current GPU Memory Usage: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")

    # Wrap main in try-except to handle any errors gracefully
    try:
        main()
    except Exception as e:
        print(f"An error occurred: {e}")
        # Release GPU memory before exiting
        clear_gpu_memory()